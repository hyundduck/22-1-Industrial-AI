{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 library 들을 load 합니다.\n",
    "import os \n",
    "from glob import glob\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image handling\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, RandomSampler, SequentialSampler, ConcatDataset\n",
    "from torchvision import models\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Augmenting library \n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Control Randomness\n",
    "import random\n",
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_dataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform = None):\n",
    "        super(train_dataset, self).__init__()\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        assert len(self.imgs) == len(self.labels)\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = np.array(Image.open(self.imgs[idx]))\n",
    "        y = label_map[self.labels[idx]]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image = X)['image']\n",
    "        else:\n",
    "            img = X\n",
    "    \n",
    "        y = torch.tensor(y, dtype = torch.long)\n",
    "        return {\n",
    "            'img' : img,\n",
    "            'label' : y\n",
    "        }\n",
    "    \n",
    "class test_dataset(Dataset):\n",
    "    def __init__(self, imgs, transform = None):\n",
    "        super(test_dataset, self).__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        # self.n_tta = n_tta\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = np.array(Image.open(self.imgs[idx]))\n",
    "        if self.transform:\n",
    "            img = self.transform(image = X)['image']\n",
    "            return img\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(128, 128, interpolation=cv2.INTER_AREA),\n",
    "    A.HorizontalFlip(),\n",
    "    A.GaussNoise(p=0.3),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit = 3, p = 0.2),\n",
    "        A.MedianBlur(blur_limit = 3, p = 0.1),\n",
    "        A.Blur(blur_limit = 3, p = 0.1),\n",
    "        ], p = 0.2),\n",
    "    A.ShiftScaleRotate(rotate_limit = 30),\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(p = 1.0),\n",
    "        A.GridDistortion(p = 1.0),\n",
    "        ], p = 0.3),\n",
    "    A.OneOf([\n",
    "        A.CLAHE(clip_limit = 2),\n",
    "        A.RandomBrightnessContrast(),\n",
    "        ], p = 0.3),\n",
    "    A.HueSaturationValue(p = 0.3),\n",
    "    A.Normalize(mean=(R_mean, G_mean, B_mean), std=(R_std, G_std, B_std)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "else_transform = A.Compose([\n",
    "    A.Resize(128, 128, interpolation=cv2.INTER_AREA),\n",
    "    A.Normalize(mean=(R_mean, G_mean, B_mean), std=(R_std, G_std, B_std)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_aug(imgs, transform, labels = None, n_aug = 5, cols = 5):\n",
    "    idx = random.randint(0, len(imgs) - 1)\n",
    "    \n",
    "    plt.imshow(np.array(Image.open(imgs[idx])))\n",
    "    \n",
    "    if labels:\n",
    "        label = labels[idx]\n",
    "        plt.title(label)\n",
    "    plt.show()\n",
    "    \n",
    "    rows = int(np.ceil(n_aug / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize = (cols * 5, rows * 5))\n",
    "\n",
    "    for i in range(n_aug):\n",
    "        img = np.array(Image.open(imgs[idx]))\n",
    "        img = transform(image = img)['image']\n",
    "        img = np.clip(img.numpy().transpose(1, 2, 0) * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "        axes.flat[i].imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "display_aug(train_imgs, train_transform, labels = train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_batch_size = 64\n",
    "valid_batch_size = 128\n",
    "test_batch_size = 127\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "save_name = f'js_effb0_b{train_batch_size}_e{epochs}_lr{learning_rate}'\n",
    "save_path = f'models/{save_name}.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score \n",
    "\n",
    "def train(model, loader, epoch, nb_epochs):\n",
    "    print(f'Epoch {epoch+1}/{nb_epochs}')\n",
    "    running_loss = 0.0\n",
    "    running_f1 = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train()\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(loader)):\n",
    "        img = batch['img'].float().to(device)\n",
    "        label = batch['label'].long().to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        #     output = model(img, csv_feature)\n",
    "        #     loss = criterion(output, label)\n",
    "\n",
    "        # scaler.scale(loss).backward() # ADDED - mixed precision + gradient clipping\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        output = model(img)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        correct = torch.eq(preds, label).sum()\n",
    "        running_loss += loss.item() \n",
    "        running_corrects+=correct\n",
    "        \n",
    "        for item in label.cpu().numpy():\n",
    "            y_true.append(item)\n",
    "        for item in preds.cpu().numpy():\n",
    "            y_pred.append(item)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = running_corrects / len(loader.dataset)\n",
    "    epoch_f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    epoch_f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    writer.add_scalar('Loss/Train', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', epoch_acc, epoch)\n",
    "    writer.add_scalar('F1_macro/Train', epoch_f1_macro, epoch)\n",
    "    writer.add_scalar('F1_weighted/Train', epoch_f1_weighted, epoch)\n",
    "    writer.add_scalar('learning_rate', optimizer.param_groups[0][\"lr\"], epoch)\n",
    "\n",
    "    print(f'Train loss: {epoch_loss:.6f}, Train ACC: {epoch_acc:.6f}, F1_macro: {epoch_f1_macro:.6f}, F1_weighted: {epoch_f1_weighted:.6f} lr: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    # scheduler.step()\n",
    "    \n",
    "def validate(model, loader, epoch, nb_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_f1 = 0.0\n",
    "    running_corrects = 0\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            img = batch['img'].float().to(device)\n",
    "            label = batch['label'].long().to(device)\n",
    "\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            # stats\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct = torch.eq(preds, label).sum()\n",
    "            running_loss += loss.item() \n",
    "            running_corrects+=correct\n",
    "    \n",
    "            for item in label.cpu().numpy():\n",
    "                y_true.append(item)\n",
    "            for item in preds.cpu().numpy():\n",
    "                y_pred.append(item)\n",
    "\n",
    "        epoch_loss = running_loss / len(loader.dataset)\n",
    "        epoch_acc = running_corrects / len(loader.dataset)\n",
    "        epoch_f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        epoch_f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        writer.add_scalar('Loss/Valid', epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Valid', epoch_acc, epoch)\n",
    "        writer.add_scalar('F1_macro/Valid', epoch_f1_macro, epoch)\n",
    "        writer.add_scalar('F1_weighted/Valid', epoch_f1_weighted, epoch)\n",
    "        \n",
    "        print(f'{running_corrects}/{len(loader.dataset)} correct' )\n",
    "        print(f'Valid loss: {epoch_loss:.6f}, Valid ACC: {epoch_acc:.6f}, F1_macro: {epoch_f1_macro:.6f}, F1_weighted: {epoch_f1_weighted:.6f}')\n",
    "        \n",
    "        val_acc_list.append(epoch_acc)\n",
    "        val_f1_macro_list.append(epoch_f1_macro)\n",
    "        val_f1_weighted_list.append(epoch_f1_weighted)\n",
    "        \n",
    "        if np.max(val_f1_macro_list) == val_f1_macro_list[-1]:  # 현재 모델이 성능 최댓값이면 저장 \n",
    "            torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs/' + save_name)\n",
    "\n",
    "val_acc_list = []\n",
    "val_f1_macro_list = []\n",
    "val_f1_weighted_list = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(model, train_loader, epoch, EPOCHS)\n",
    "    validate(model, val_loader, epoch, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y_true = []\n",
    "val_y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "        img = batch['img'].float().to(device)\n",
    "        csv_feature = batch['csv_feature'].to(device)\n",
    "        label = batch['label'].long().to(device)\n",
    "        output = model(img, csv_feature)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        \n",
    "        for item in label.cpu().numpy():\n",
    "            val_y_true.append(item)\n",
    "        for item in preds.cpu().numpy():\n",
    "            val_y_pred.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(\n",
    "    confusion_matrix(\n",
    "        val_y_true,\n",
    "        val_y_pred,\n",
    "        normalize = 'true'), \n",
    "        annot=True, cmap='coolwarm'\n",
    "        )\n",
    "plt.title(\"CONFUSION MATRIX of the val dataset\", fontsize=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
